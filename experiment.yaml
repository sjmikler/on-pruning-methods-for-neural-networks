Global:
    repeat: 2  # e.g. 1, 2, 1, 2
    queue: null # specify path for queue file, if null keeps queue unaccessible in memory

# DEFAULT CONFIG:
Name: null # with name=None, training will be skipped
Repeat: 2  # e.g. 1, 1, 2, 2
Module: modules.pruning
YamlLog: eval f"{directory}/{Name}/{RND_IDX}/{REP}.yaml"

precision: 16 # 16 for mixed precision or anything else for default 32 bit
directory: data/dummy_tests
full_path: eval f"{directory}/{Name}/{RND_IDX}/{REP}" # REP is a value from [0, REPEAT-1] interval
checkpoint: eval f"{directory}/{Name}/{RND_IDX}/{REP}.h5" # RND_IDX is automatically assigned in `run.py`

steps: 80000
steps_per_epoch: 2000

dataset: cifar10 # mnist|cifar10|cifar100 available
dataset_config:
    train_batch_size: 128
    valid_batch_size: 512
    padding: reflect

optimizer: tf.optimizers.SGD
optimizer_config:
    learning_rate: tf.optimizers.schedules.PiecewiseConstantDecay([32000, 48000, 64000], [0.1, 0.02, 0.004, 0.0008])
    momentum: 0.9
    nesterov: True

model: WRN
model_config:
    N: 16
    K: 4
    input_shape: [32, 32, 3]
    n_classes: 10
    l2_reg: 2e-4

pruning: none # none|random|l1|snip|grasp
pruning_config:
    sparsity: 0
    structure: false # true for classic structural pruning OR int N for enforcing N groups OR false

# EXPERIMENTS:
---
Name: WRN16-4-short-training
steps: 2000
---
Name: WRN16-4
